{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9124ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import anndata as ad\n",
    "from torch_geometric.loader import DataLoader\n",
    "import scanpy as sc\n",
    "from scregat.data_process import prepare_model_input,sum_counts,plot_edge, ATACGraphDataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c39205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你要研究的barcode\n",
    "test_cell = pd.read_csv('./addgene_add_crc_hic_eqtl_df_mat_RIS_index.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff024f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import anndata as ad\n",
    "from torch_geometric.loader import DataLoader\n",
    "import scanpy as sc\n",
    "from scregat.data_process import prepare_model_input,sum_counts,plot_edge, ATACGraphDataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('./dataset_atac.pkl', 'rb') as f:\n",
    "    dataset_atac = pickle.load(f)\n",
    "\n",
    "dataset_graph = ATACGraphDataset('./input_graph/')\n",
    "seq_vec = torch.load('./seq.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = dataset_atac.array_peak\n",
    "torch_vector = torch.zeros(len(data))\n",
    "for idx, item in enumerate(data):\n",
    "    if item.startswith('chr'):\n",
    "        torch_vector[idx] = 0 \n",
    "    else:\n",
    "        torch_vector[idx] = 1 \n",
    "print(torch_vector)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 现在我们可以生成修改原始图\n",
    "train_graph = []\n",
    "test_graph = []\n",
    "\n",
    "for i, sample in tqdm(enumerate(dataset_graph), total=len(dataset_graph), desc='Processing samples'):\n",
    "# 此时的sample代表一个细胞的图\n",
    "    seq_data = seq_vec\n",
    "    sample.seq_data = seq_data\n",
    "    sample.id_vec = torch_vector\n",
    "  \n",
    "    # 请注意，如果你要添加边\n",
    "    # 可以直接修改 sample.edge_index\n",
    "    # sample.edge_index[1]是promoter节点的索引\n",
    "    # sample.edge_index[0]是promoter节点对应的peak节点的索引\n",
    "    # 每个索引代表的peak或promoter节点的含义可以通过dataset.array_peak查询\n",
    "    # 比如\n",
    "    # dataset_atac.array_peak\n",
    "    # array(['ACAP3', 'VWA1', 'PRDM16', ..., 'chrX-154762391-154763458',\n",
    "    # \n",
    "    # sample.edge_index\n",
    "    # tensor([[21068, 21083, 21084,  ..., 56014, 56041, 56043],\n",
    "    #     [    0,     1,     1,  ...,  1528,  1529,  1529]], device='cuda:3')\n",
    "    # 0就代表了dataset_atac.array_peak[0]: 'ACAP3'\n",
    "    # 21068就代表了dataset_atac.array_peak[21068] : 'chr1-1307926-1308733'\n",
    "    # \n",
    "    # 假设我们要添加基因'ACAP3'和'chr1-1307926-1308733'，可以：\n",
    "    # sample.edge_index[0] = torch.cat((sample.edge_index[0], torch.tensor([21068])))\n",
    "    # sample.edge_index[1] = torch.cat((sample.edge_index[1], torch.tensor([0])))\n",
    "    # 你当然也可以直接删除边，scReGAT采用的架构可以无视图的点边结构\n",
    "\n",
    "\n",
    "    # 当然我们也可以在这里直接修改细胞的target基因表达y_exp\n",
    "    # sample.y_exp = torch.tensor([新的值array])就可以了\n",
    "    # 意味着我们可以轻松改成单细胞表达\n",
    "\n",
    "    if sample.cell in test_cell:\n",
    "        test_graph.append(sample)\n",
    "    else:\n",
    "        train_graph.append(sample)\n",
    "\n",
    "\n",
    "\n",
    "# 下面是模型，这是KL散度版本\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class SCReGAT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_input_dim=2,\n",
    "                 node_output_dim=8,\n",
    "                 edge_embedding_dim=8,\n",
    "                 hidden_channels=8,\n",
    "                 gat_input_channels=8,\n",
    "                 gat_hidden_channels=8,\n",
    "                 seq_dim=768,\n",
    "                 seq2node_dim=1,\n",
    "                 max_tokens=1024,\n",
    "                 dropout=0.2,\n",
    "                 num_head_1=8,\n",
    "                 num_head_2=8):\n",
    "        super(SCReGAT, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        # Sequence transformation layer (currently commented out in forward)\n",
    "        self.NN_seq = nn.Sequential(\n",
    "            nn.Linear(seq_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, seq2node_dim),\n",
    "        )\n",
    "\n",
    "        # Node feature transformation with BatchNorm\n",
    "        self.NN_node = nn.Sequential(\n",
    "            nn.Linear(node_input_dim, 128),\n",
    "            nn.LayerNorm(128),  # BatchNorm added after Linear\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),  # BatchNorm added\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, node_output_dim),\n",
    "        )\n",
    "\n",
    "        # Edge feature transformation with BatchNorm\n",
    "        self.NN_edge = nn.Sequential(\n",
    "            nn.Linear(2, 12),\n",
    "            nn.LayerNorm(12),  # BatchNorm added\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(12, edge_embedding_dim)\n",
    "        )\n",
    "\n",
    "        # GAT layers with dropout\n",
    "        self.NN_conv1 = GATConv(node_output_dim, hidden_channels, heads=num_head_1, dropout=dropout, edge_dim=edge_embedding_dim, add_self_loops=False)\n",
    "        self.NN_flatten1 = nn.Linear(num_head_1 * hidden_channels, hidden_channels)\n",
    "\n",
    "        self.NN_conv2 = GATConv(hidden_channels, hidden_channels, heads=num_head_2, dropout=dropout, add_self_loops=False)\n",
    "        self.NN_flatten2 = nn.Linear(num_head_2 * hidden_channels, hidden_channels)\n",
    "\n",
    "        # Adapters and dropout layers\n",
    "        self.NN_adapter_barcode = nn.Linear(hidden_channels, 1)\n",
    "        self.NN_adapter_bulk = nn.Linear(hidden_channels, 1, bias=True)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky = nn.LeakyReLU()\n",
    "\n",
    "\n",
    "    def forward(self, seq_data, raw_x, edge_index, edge_tf, batch, gene_num, tag):\n",
    "        data = raw_x\n",
    "        seq_data = self.NN_seq(seq_data)\n",
    "        data = torch.cat((data, seq_data), dim=1)\n",
    "        data = self.NN_node(data)\n",
    "        \n",
    "        hidden_edge_input = torch.cat((raw_x[edge_index[0]], raw_x[edge_index[1]]), dim=1)\n",
    "        hidden_edge = self.NN_edge(hidden_edge_input).sigmoid()\n",
    "    \n",
    "        data, atten_w1 = self.NN_conv1(data, edge_index, edge_attr=hidden_edge, return_attention_weights=True)\n",
    "        data_1 = self.leaky(self.NN_flatten1(data))\n",
    "\n",
    "        data_2, atten_w2 = self.NN_conv2(data_1, edge_tf, return_attention_weights=True)\n",
    "        data_2 = self.leaky(self.NN_flatten2(data_2))\n",
    "        \n",
    "        data = data_1 + data_2\n",
    "        \n",
    "        # data = self.NN_adapter_bulk(data)\n",
    "        self.data = data\n",
    "        gene_out = -F.log_softmax(data, dim=1)[:, 0]\n",
    "        return gene_out, atten_w1\n",
    "\n",
    "\n",
    "# 你懂的\n",
    "model = SCReGAT()\n",
    "train_loader = DataLoader(train_graph, batch_size=1, shuffle=True)\n",
    "gene_num = len(sample.y_exp)\n",
    "\n",
    "\n",
    "# 训练部分\n",
    "\n",
    "# 假设你已经定义了 model, train_graph 等\n",
    "device = 'cuda:3'\n",
    "model.to(device)\n",
    "\n",
    "# 使用 KL 散度损失\n",
    "loss_exp = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epoch = 20\n",
    "drop_edge_rate = 0.1\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()  \n",
    "    \n",
    "# 以下是随机抽样训练，你也可以用\n",
    "#     # 随机跳过部分 epoch\n",
    "#     if random.randint(1, 4) != 2:\n",
    "#         continue\n",
    "#     else:\n",
    "#         random.shuffle(train_graph)\n",
    "#         train_loader = DataLoader(train_graph[:500], batch_size=1, shuffle=True)\n",
    "\n",
    "    running_loss = 0.0  \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epoch}\", unit=\"batch\")  \n",
    "    count = 0\n",
    "\n",
    "    for sample in progress_bar:\n",
    "        gene_num = sample.y_exp.shape[0]\n",
    "        optimizer.zero_grad()  \n",
    "\n",
    "        # 执行前向传播\n",
    "        gene_pre, atten = model(\n",
    "            sample.seq_data.to(device),\n",
    "            sample.x.to(device), \n",
    "            sample.edge_index.to(device),\n",
    "            sample.edge_tf.T.to(device), \n",
    "            sample.batch.to(device), \n",
    "            gene_num, \n",
    "            sample.id_vec.to(device)\n",
    "        )\n",
    "\n",
    "        # 我们只考虑那些活性 > 0的promoter对应的基因表达，因为这样可以避免将0纳入\n",
    "        index = torch.where(sample.x[:gene_num] > 0)[0]\n",
    "        gene_pre_log_prob = torch.log_softmax(gene_pre[index].flatten(), dim=-1)  \n",
    "        loss = -loss_exp(gene_pre[index].flatten(), sample.y_exp[index].to(device)) \n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=running_loss / (progress_bar.n + 1))  # 更新进度条的损失值\n",
    "       \n",
    "    print(f\"Epoch [{epoch+1}/{num_epoch}], Average Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# 测试\n",
    "test_loader = DataLoader(test_graph, batch_size=1, shuffle=False, pin_memory=True)\n",
    "model.eval()  # 切换到评估模式\n",
    "test_loss = 0.0  # 初始化测试集的总损失\n",
    "\n",
    "# 使用 torch.no_grad() 禁用梯度计算\n",
    "device = 'cuda:3'\n",
    "cell_type = []\n",
    "with torch.no_grad():\n",
    "    cell_link_mt = []\n",
    "    for idx, sample in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Processing Batches\"):\n",
    "        gene_pre, atten = model(sample.seq_data.to(device),\n",
    "                                  sample.x.to(device), \n",
    "                                  sample.edge_index.to(device), \n",
    "                                  sample.edge_tf.T.to(device), \n",
    "                                  sample.batch.to(device), gene_num, sample.id_vec.to(device))\n",
    "        cell_type.append(sample.y.item())\n",
    "\n",
    "        temp = torch.mean(atten[1], dim=1) * sample.to(device).x[atten[0][1].cpu()].flatten()\n",
    "        cell_link_mt.append(temp)\n",
    "\n",
    "# cell_link_mt 这个矩阵是[n_cell * n_edge]\n",
    "# 其中边的与sample.edge_index的索引完全一致\n",
    "# cell_type是你用于测试用的细胞类别"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
